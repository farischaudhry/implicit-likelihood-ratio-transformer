{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c1901b",
   "metadata": {},
   "source": [
    "# Implicit Statistical Reasoning in Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7805d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import logging \n",
    "import pandas as pd\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Logging format: save logs to logs/ folder + console output\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/experiment.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Random seeds\n",
    "def set_seed(seed: int = 0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(0)\n",
    "\n",
    "\n",
    "def set_plotting_style():\n",
    "    \"\"\"Sets up standard plotting style.\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'text.usetex': False,\n",
    "        'font.family': 'serif',\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 11,\n",
    "        'ytick.labelsize': 11,\n",
    "        'legend.fontsize': 11,\n",
    "        'figure.figsize': (10, 7),\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3,\n",
    "        'lines.linewidth': 2.5,\n",
    "    })\n",
    "set_plotting_style()\n",
    "\n",
    "\n",
    "# PyTorch device selection\n",
    "DEVICE = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() \n",
    "    else 'mps' if torch.backends.mps.is_available() \n",
    "    else 'cpu'\n",
    ")\n",
    "logging.info(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24272866",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Controls the generation of synthetic tasks.\"\"\"\n",
    "    d: int = 16  # Input dimension\n",
    "    n_ctx: int = 32  # Number of context pairs\n",
    "    sigma_k: float = 3.0  # Task A: Shift magnitude\n",
    "    sigma_min: float = 0.5  # Task B: Variance lower bound\n",
    "    sigma_max: float = 3.0  # Task B: Variance upper bound\n",
    "    train_episodes: int = 50000  # Size of training dataset\n",
    "    val_episodes: int = 5000  # Size of validation dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    \"\"\"Controls the optimization loop.\"\"\"\n",
    "    seeds: list = [0, 1, 2]\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 10\n",
    "    lr: float = 3e-4\n",
    "    device: torch.device = DEVICE\n",
    "\n",
    "\n",
    "# Initialize Global Configs\n",
    "data_cfg = DataConfig()\n",
    "train_cfg = TrainConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c67d8",
   "metadata": {},
   "source": [
    "## Sampling Tasks\n",
    "\n",
    "### Dataset Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e22acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_episode(\n",
    "    sample_task_params_fn: callable,\n",
    "    sample_data_fn: callable,\n",
    "    n_ctx: int,\n",
    "    d: int,\n",
    "    **task_kwargs\n",
    "):\n",
    "    \"\"\"Generic episode constructor.\"\"\"\n",
    "    task_params = sample_task_params_fn(d=d, **task_kwargs)\n",
    "\n",
    "    x_ctx, y_ctx = sample_data_fn(task_params, n_ctx, d)\n",
    "    x_q, y_q = sample_data_fn(task_params, 1, d)\n",
    "\n",
    "    return {\n",
    "        'context_x': x_ctx,\n",
    "        'context_y': y_ctx,\n",
    "        'query_x': x_q[0],\n",
    "        'query_y': y_q[0],\n",
    "        'task_params': task_params,\n",
    "    }\n",
    "\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for generating episodes on-the-fly.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_task_params_fn,\n",
    "        sample_data_fn,\n",
    "        n_ctx: int,\n",
    "        d: int,\n",
    "        num_episodes: int,\n",
    "        device: torch.device = torch.device('cpu'),\n",
    "        **task_kwargs\n",
    "    ):\n",
    "        self.sample_task_params_fn = sample_task_params_fn\n",
    "        self.sample_data_fn = sample_data_fn\n",
    "        self.n_ctx = n_ctx\n",
    "        self.d = d\n",
    "        self.num_episodes = num_episodes\n",
    "        self.task_kwargs = dict(task_kwargs)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_episodes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Each batch from  yields:\n",
    "        {\n",
    "        'context_x': (B, n_ctx, d),\n",
    "        'context_y': (B, n_ctx),\n",
    "        'query_x':   (B, d),\n",
    "        'query_y':   (B,),\n",
    "        }\n",
    "        \"\"\"\n",
    "        episode = make_episode(\n",
    "            sample_task_params_fn=self.sample_task_params_fn,\n",
    "            sample_data_fn=self.sample_data_fn,\n",
    "            n_ctx=self.n_ctx,\n",
    "            d=self.d,\n",
    "            **self.task_kwargs\n",
    "        )\n",
    "\n",
    "        # Convert to torch tensors where appropriate\n",
    "        return {\n",
    "            'context_x': torch.tensor(episode['context_x'], dtype=torch.float32, device=self.device),\n",
    "            'context_y': torch.tensor(episode['context_y'], dtype=torch.long, device=self.device),\n",
    "            'query_x': torch.tensor(episode['query_x'], dtype=torch.float32, device=self.device),\n",
    "            'query_y': torch.tensor(episode['query_y'], dtype=torch.long, device=self.device),\n",
    "            'task_params': episode['task_params'],  # keep as Python object\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cddecb5",
   "metadata": {},
   "source": [
    "### Task A: Shifted Mean Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_task_A_params(d: int, sigma_k: float) -> dict:\n",
    "    \"\"\"\n",
    "    Samples task parameters for Task A: Mean Discrimination.\n",
    "\n",
    "    Arguments:\n",
    "    d: data dimension\n",
    "    sigma_k: standard deviation of the shift, k\n",
    "\n",
    "    Returns (mu, k).\n",
    "    \"\"\"\n",
    "    mu = np.random.randn(d)\n",
    "    mu /= np.linalg.norm(mu) \n",
    "\n",
    "    k = np.random.randn(d) * sigma_k\n",
    "    return {'mu': mu, 'k': k}\n",
    "\n",
    "\n",
    "def sample_task_A_data(task_params: dict, n: int, d: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Samples labeled data from mean discrimination task.\n",
    "    Returns {x, y}_1^n with y in {0,1}.\n",
    "    \"\"\"\n",
    "    mu, k = task_params['mu'], task_params['k']\n",
    "    y = np.random.randint(0, 2, size=n)\n",
    "    means = np.where(y[:, None] == 1, mu + k, -mu + k)\n",
    "    x = means + np.random.randn(n, d)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def task_A_llr(x: np.ndarray, mu: np.ndarray, k: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Bayes-optimal log-likelihood ratio for mean discrimination.\n",
    "    \"\"\"\n",
    "    return 2.0 * (x - k) @ mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f4fd0",
   "metadata": {},
   "source": [
    "### Task B: Variance Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec298962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_task_B_params(d: int, sigma_min: float = 0.5, sigma_max: float = 3.0) -> dict:\n",
    "    \"\"\"\n",
    "    Samples variances (sigma_0, sigma_1) from uniform distributions.\n",
    "    \"\"\"\n",
    "    sigma_0 = np.random.uniform(sigma_min, sigma_max)\n",
    "    sigma_1 = np.random.uniform(sigma_min, sigma_max)\n",
    "    return {'sigma_0': sigma_0, 'sigma_1': sigma_1}\n",
    "\n",
    "\n",
    "def sample_task_B_data(task_params: dict, n: int, d: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Samples labeled data from variance discrimination task.\n",
    "    \"\"\"\n",
    "    sigma_0 = task_params['sigma_0']\n",
    "    sigma_1 = task_params['sigma_1']\n",
    "\n",
    "    y = np.random.randint(0, 2, size=n)\n",
    "    sigmas = np.where(y == 1, sigma_1, sigma_0)\n",
    "\n",
    "    x = np.random.randn(n, d) * sigmas[:, None]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def task_B_llr(x: np.ndarray, sigma_0: float, sigma_1: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Bayes-optimal log-likelihood ratio for variance discrimination.\n",
    "    \"\"\"\n",
    "    d = x.shape[1]\n",
    "    quad = np.sum(x**2, axis=1)\n",
    "\n",
    "    return (\n",
    "        0.5 * (1.0 / sigma_0**2 - 1.0 / sigma_1**2) * quad\n",
    "        + d * np.log(sigma_0 / sigma_1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636bd7f",
   "metadata": {},
   "source": [
    "### Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea24e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "train_loader_A = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_A_params,\n",
    "        sample_data_fn=sample_task_A_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.train_episodes,\n",
    "        sigma_k=data_cfg.sigma_k,\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "val_loader_A = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_A_params,\n",
    "        sample_data_fn=sample_task_A_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.val_episodes,\n",
    "        sigma_k=data_cfg.sigma_k,\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=False, drop_last=False\n",
    ")\n",
    "val_loader_A_OOD = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_A_params,\n",
    "        sample_data_fn=sample_task_A_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.val_episodes,\n",
    "        sigma_k=100,  # OOD shift magnitude (much larger than training)\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=False, drop_last=False\n",
    ")\n",
    "train_loader_B = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_B_params,\n",
    "        sample_data_fn=sample_task_B_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.train_episodes,\n",
    "        sigma_min=data_cfg.sigma_min,\n",
    "        sigma_max=data_cfg.sigma_max,\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "val_loader_B = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_B_params,\n",
    "        sample_data_fn=sample_task_B_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.val_episodes,\n",
    "        sigma_min=data_cfg.sigma_min,\n",
    "        sigma_max=data_cfg.sigma_max,\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e132f22",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea053c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: nn.Module, batch: dict, optimizer: optim.Optimizer, loss_fn: nn.Module):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = model(\n",
    "        context_x=batch['context_x'],\n",
    "        context_y=batch['context_y'],\n",
    "        query_x=batch['query_x'],\n",
    "    )\n",
    "\n",
    "    target = batch['query_y'].float().view(-1, 1) \n",
    "    loss = loss_fn(logits, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = (logits > 0).long()\n",
    "        acc = (preds == batch['query_y']).float().mean().item()\n",
    "\n",
    "    return loss.item(), acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_step(model: nn.Module, batch: dict, loss_fn: nn.Module):\n",
    "    model.eval()\n",
    "\n",
    "    logits = model(\n",
    "        context_x=batch['context_x'],\n",
    "        context_y=batch['context_y'],\n",
    "        query_x=batch['query_x'],\n",
    "    )\n",
    "\n",
    "    target = batch['query_y'].float().view(-1, 1)\n",
    "    loss = loss_fn(logits, target)\n",
    "    \n",
    "    preds = (logits > 0).long()\n",
    "    acc = (preds == target.long()).float().mean().item()\n",
    "\n",
    "    return loss.item(), acc, logits.cpu(), batch['query_y'].cpu()\n",
    "\n",
    "\n",
    "def run_epoch(model: nn.Module, dataloader: DataLoader, optimizer: optim.Optimizer = None):\n",
    "    is_train = optimizer is not None\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "    all_logits, all_labels = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        if is_train:\n",
    "            loss, acc = train_step(model, batch, optimizer, loss_fn)\n",
    "        else:\n",
    "            loss, acc, logits, labels = eval_step(model, batch, loss_fn)\n",
    "            all_logits.append(logits)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_acc = total_acc / len(dataloader)\n",
    "\n",
    "    metrics = {'loss': avg_loss, 'acc': avg_acc}\n",
    "\n",
    "    if not is_train:\n",
    "        logits = torch.cat(all_logits)\n",
    "        labels = torch.cat(all_labels)\n",
    "        metrics['logits'] = logits\n",
    "        metrics['labels'] = labels\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    ") -> list[dict]:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_metrics = run_epoch(model, train_loader, optimizer)\n",
    "        val_metrics = run_epoch(model, val_loader)\n",
    "\n",
    "        # Store metrics for this epoch\n",
    "        history.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_metrics['loss'],\n",
    "            'train_acc': train_metrics['acc'],\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_acc': val_metrics['acc']\n",
    "        })\n",
    "\n",
    "        logging.info(\n",
    "            f\"[Epoch {epoch:02d}] \"\n",
    "            f\"Train loss={train_metrics['loss']:.4f}, acc={train_metrics['acc']:.3f} | \"\n",
    "            f\"Val loss={val_metrics['loss']:.4f}, acc={val_metrics['acc']:.3f}\"\n",
    "        )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def run_multiseed_experiment(\n",
    "    model_class: type[nn.Module],\n",
    "    model_kwargs: dict,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    seeds: list[int] | None,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    "    device: torch.device = torch.device('cpu')\n",
    ") -> pd.DataFrame:\n",
    "    if seeds is None:\n",
    "        seeds = [0, 1, 2]\n",
    "    all_results = []\n",
    "    model_name = model_class.__name__\n",
    "\n",
    "    logging.info(f'Running experiment for model: {model_name} with seeds: {seeds}')\n",
    "    \n",
    "    for seed in seeds:\n",
    "        logging.info(f'Starting training with seed: {seed}')\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Re-initialize Model Fresh \n",
    "        model = model_class(**model_kwargs).to(device)\n",
    "        history = train_model(model, train_loader, val_loader, epochs=epochs, lr=lr)\n",
    "\n",
    "        # Add metadata and store\n",
    "        for epoch_data in history:\n",
    "            epoch_data['seed'] = seed\n",
    "            epoch_data['model'] = model_name\n",
    "            all_results.append(epoch_data)\n",
    "\n",
    "    set_seed(0) \n",
    "    # Convert to pd df \n",
    "    return pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b2afc",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad361953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticBaseline(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_hidden, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, context_x, context_y, query_x):\n",
    "        # Context is intentionally ignored\n",
    "        return self.net(query_x)\n",
    "\n",
    "\n",
    "class ICLTransformer(nn.Module):\n",
    "    def __init__(self, d_in, d_model=128, n_layers=2, n_heads=4, max_len=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings\n",
    "        # Projects input data x -> d_model\n",
    "        self.x_embed = nn.Linear(d_in, d_model)\n",
    "        # Embeds binary labels y -> d_model\n",
    "        self.y_embed = nn.Embedding(2, d_model)\n",
    "        # Learned positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model) * 0.02)\n",
    "        \n",
    "        # Transformer Encoder (GPT-style)\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=4*d_model, \n",
    "            dropout=0.0, # No dropout for synthetic tasks required\n",
    "            batch_first=True,\n",
    "            activation='gelu',\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        \n",
    "        # Prediction Head\n",
    "        self.head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, context_x, context_y, query_x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            context_x: (B, N, d)\n",
    "            context_y: (B, N)\n",
    "            query_x:   (B, d)\n",
    "        \"\"\"\n",
    "        B, N, _ = context_x.shape\n",
    "        device = context_x.device\n",
    "        \n",
    "        # Embed inputs \n",
    "        ctx_x_emb = self.x_embed(context_x)  # (B, N, d_model)\n",
    "        qry_x_emb = self.x_embed(query_x.unsqueeze(1))  # (B, 1, d_model)\n",
    "        ctx_y_emb = self.y_embed(context_y)  # (B, N, d_model)\n",
    "        \n",
    "        # Interleave Sequence \n",
    "        # Sequence: [x1, y1, x2, y2, ..., xN, yN, x_query]\n",
    "        # Total length = 2*N + 1\n",
    "        seq_len = 2*N + 1\n",
    "        seq_emb = torch.zeros(B, seq_len, self.d_model, device=device)\n",
    "        \n",
    "        # Evens are X, Odds are Y\n",
    "        seq_emb[:, 0:2*N:2, :] = ctx_x_emb\n",
    "        seq_emb[:, 1:2*N:2, :] = ctx_y_emb\n",
    "        # Last token is Query X\n",
    "        seq_emb[:, -1, :] = qry_x_emb.squeeze(1)\n",
    "        \n",
    "        # Add Position & Forward\n",
    "        seq_emb = seq_emb + self.pos_embed[:, :seq_len, :]\n",
    "        \n",
    "        out = self.transformer(seq_emb)\n",
    "        \n",
    "        # Predict on last token\n",
    "        # We only care about the representation of x_query\n",
    "        last_token = out[:, -1, :]\n",
    "        return self.head(last_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd77c1",
   "metadata": {},
   "source": [
    "## PART I: The Neccessity of Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b73448",
   "metadata": {},
   "source": [
    "## PART II: Recovery of Optimal Tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "implicit-likelihood-ratio-transformer (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
