{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c1901b",
   "metadata": {},
   "source": [
    "# Implicit Statistical Reasoning in Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7805d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import logging \n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from dataclasses import dataclass, field\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Logging format: save logs to logs/ folder + console output\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/experiment.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Random seeds\n",
    "def set_seed(seed: int = 0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(0)\n",
    "\n",
    "\n",
    "def set_plotting_style():\n",
    "    \"\"\"Sets up standard plotting style.\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'text.usetex': False,\n",
    "        'font.family': 'serif',\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 11,\n",
    "        'ytick.labelsize': 11,\n",
    "        'legend.fontsize': 11,\n",
    "        'figure.figsize': (10, 7),\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3,\n",
    "        'lines.linewidth': 2.5,\n",
    "    })\n",
    "set_plotting_style()\n",
    "\n",
    "\n",
    "# PyTorch device selection\n",
    "DEVICE = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() \n",
    "    else 'mps' if torch.backends.mps.is_available() \n",
    "    else 'cpu'\n",
    ")\n",
    "logging.info(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24272866",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Controls the generation of synthetic tasks.\"\"\"\n",
    "    d: int = 16  # Input dimension\n",
    "    n_ctx: int = 32  # Number of context pairs\n",
    "    sigma_k: float = 3.0  # Task A: Shift magnitude\n",
    "    sigma_min: float = 0.5  # Task B: Variance lower bound\n",
    "    sigma_max: float = 3.0  # Task B: Variance upper bound\n",
    "    train_episodes: int = 50000  # Size of training dataset\n",
    "    val_episodes: int = 5000  # Size of validation dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    \"\"\"Controls the optimization loop.\"\"\"\n",
    "    seeds: list[int] = field(default_factory=lambda: [0, 1, 2])\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 20\n",
    "    lr: float = 3e-4\n",
    "    device: torch.device = DEVICE\n",
    "\n",
    "\n",
    "# Initialize Global Configs\n",
    "data_cfg = DataConfig()\n",
    "train_cfg = TrainConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c67d8",
   "metadata": {},
   "source": [
    "## Sampling Tasks\n",
    "\n",
    "### Dataset Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e22acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_episode(\n",
    "    sample_task_params_fn: callable,\n",
    "    sample_data_fn: callable,\n",
    "    n_ctx: int,\n",
    "    d: int,\n",
    "    **task_kwargs\n",
    "):\n",
    "    \"\"\"Generic episode constructor.\"\"\"\n",
    "    task_params = sample_task_params_fn(d=d, **task_kwargs)\n",
    "\n",
    "    x_ctx, y_ctx = sample_data_fn(task_params, n_ctx, d)\n",
    "    x_q, y_q = sample_data_fn(task_params, 1, d)\n",
    "\n",
    "    return {\n",
    "        'context_x': x_ctx,\n",
    "        'context_y': y_ctx,\n",
    "        'query_x': x_q[0],\n",
    "        'query_y': y_q[0],\n",
    "        'task_params': task_params,\n",
    "    }\n",
    "\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for generating episodes on-the-fly.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_task_params_fn,\n",
    "        sample_data_fn,\n",
    "        n_ctx: int,\n",
    "        d: int,\n",
    "        num_episodes: int,\n",
    "        device: torch.device = torch.device('cpu'),\n",
    "        **task_kwargs\n",
    "    ):\n",
    "        self.sample_task_params_fn = sample_task_params_fn\n",
    "        self.sample_data_fn = sample_data_fn\n",
    "        self.n_ctx = n_ctx\n",
    "        self.d = d\n",
    "        self.num_episodes = num_episodes\n",
    "        self.task_kwargs = dict(task_kwargs)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_episodes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Each batch from  yields:\n",
    "        {\n",
    "        'context_x': (B, n_ctx, d),\n",
    "        'context_y': (B, n_ctx),\n",
    "        'query_x':   (B, d),\n",
    "        'query_y':   (B,),\n",
    "        }\n",
    "        \"\"\"\n",
    "        episode = make_episode(\n",
    "            sample_task_params_fn=self.sample_task_params_fn,\n",
    "            sample_data_fn=self.sample_data_fn,\n",
    "            n_ctx=self.n_ctx,\n",
    "            d=self.d,\n",
    "            **self.task_kwargs\n",
    "        )\n",
    "\n",
    "        # Convert to torch tensors where appropriate\n",
    "        return {\n",
    "            'context_x': torch.as_tensor(episode['context_x'], dtype=torch.float32, device=self.device),\n",
    "            'context_y': torch.as_tensor(episode['context_y'], dtype=torch.long, device=self.device),\n",
    "            'query_x': torch.as_tensor(episode['query_x'], dtype=torch.float32, device=self.device),\n",
    "            'query_y': torch.as_tensor(episode['query_y'], dtype=torch.float32, device=self.device),\n",
    "            'task_params': episode['task_params'],  # keep as Python object\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cddecb5",
   "metadata": {},
   "source": [
    "### Task A: Shifted Mean Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_task_A_params(d: int, sigma_k: float) -> dict:\n",
    "    \"\"\"\n",
    "    Samples task parameters for Task A: Mean Discrimination.\n",
    "\n",
    "    Arguments:\n",
    "    d: data dimension\n",
    "    sigma_k: standard deviation of the shift, k\n",
    "\n",
    "    Returns (mu, k).\n",
    "    \"\"\"\n",
    "    mu = np.random.randn(d)\n",
    "    mu /= np.linalg.norm(mu) \n",
    "\n",
    "    k = np.random.randn(d) * sigma_k\n",
    "    return {'mu': mu, 'k': k}\n",
    "\n",
    "\n",
    "def sample_task_A_data(task_params: dict, n: int, d: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Samples labeled data from mean discrimination task.\n",
    "    Returns {x, y}_1^n with y in {0,1}.\n",
    "    \"\"\"\n",
    "    mu, k = task_params['mu'], task_params['k']\n",
    "    y = np.random.randint(0, 2, size=n)\n",
    "    means = np.where(y[:, None] == 1, mu + k, -mu + k)\n",
    "    x = means + np.random.randn(n, d)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def task_A_llr(x: np.ndarray, mu: np.ndarray, k: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Bayes-optimal log-likelihood ratio for mean discrimination.\n",
    "    \"\"\"\n",
    "    return 2.0 * (x - k) @ mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f4fd0",
   "metadata": {},
   "source": [
    "### Task B: Variance Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec298962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_task_B_params(d: int, sigma_min: float = 0.5, sigma_max: float = 3.0) -> dict:\n",
    "    \"\"\"\n",
    "    Samples variances (sigma_0, sigma_1) from uniform distributions.\n",
    "    \"\"\"\n",
    "    sigma_0 = np.random.uniform(sigma_min, sigma_max)\n",
    "    sigma_1 = np.random.uniform(sigma_min, sigma_max)\n",
    "    return {'sigma_0': sigma_0, 'sigma_1': sigma_1}\n",
    "\n",
    "\n",
    "def sample_task_B_data(task_params: dict, n: int, d: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Samples labeled data from variance discrimination task.\n",
    "    \"\"\"\n",
    "    sigma_0 = task_params['sigma_0']\n",
    "    sigma_1 = task_params['sigma_1']\n",
    "\n",
    "    y = np.random.randint(0, 2, size=n)\n",
    "    sigmas = np.where(y == 1, sigma_1, sigma_0)\n",
    "\n",
    "    x = np.random.randn(n, d) * sigmas[:, None]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def task_B_llr(x: np.ndarray, sigma_0: float, sigma_1: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Bayes-optimal log-likelihood ratio for variance discrimination.\n",
    "    \"\"\"\n",
    "    d = x.shape[1]\n",
    "    quad = np.sum(x**2, axis=1)\n",
    "\n",
    "    return (\n",
    "        0.5 * (1.0 / sigma_0**2 - 1.0 / sigma_1**2) * quad\n",
    "        + d * np.log(sigma_0 / sigma_1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636bd7f",
   "metadata": {},
   "source": [
    "### Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea24e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "train_loader_A_no_nuisance = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_A_params,\n",
    "        sample_data_fn=sample_task_A_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.train_episodes,\n",
    "        sigma_k=0.0,  # No nuisance shift\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "val_loader_A_no_nuisance = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_A_params,\n",
    "        sample_data_fn=sample_task_A_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.val_episodes,\n",
    "        sigma_k=0.0,  # No nuisance shift\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=False, drop_last=False\n",
    ")\n",
    "train_loader_A = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_A_params,\n",
    "        sample_data_fn=sample_task_A_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.train_episodes,\n",
    "        sigma_k=data_cfg.sigma_k,\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "val_loader_A = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_A_params,\n",
    "        sample_data_fn=sample_task_A_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.val_episodes,\n",
    "        sigma_k=data_cfg.sigma_k,\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=False, drop_last=False\n",
    ")\n",
    "val_loader_A_OOD = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_A_params,\n",
    "        sample_data_fn=sample_task_A_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.val_episodes,\n",
    "        sigma_k=3*data_cfg.sigma_k,  # OOD nuisance variables\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=False, drop_last=False\n",
    ")\n",
    "train_loader_B = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_B_params,\n",
    "        sample_data_fn=sample_task_B_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.train_episodes,\n",
    "        sigma_min=data_cfg.sigma_min,\n",
    "        sigma_max=data_cfg.sigma_max,\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "val_loader_B = DataLoader(\n",
    "    EpisodeDataset(\n",
    "        sample_task_params_fn=sample_task_B_params,\n",
    "        sample_data_fn=sample_task_B_data,\n",
    "        n_ctx=data_cfg.n_ctx,\n",
    "        d=data_cfg.d,\n",
    "        num_episodes=data_cfg.val_episodes,\n",
    "        sigma_min=data_cfg.sigma_min,\n",
    "        sigma_max=data_cfg.sigma_max,\n",
    "        device=train_cfg.device\n",
    "    ),\n",
    "    batch_size=train_cfg.batch_size, shuffle=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e132f22",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea053c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: nn.Module, batch: dict, optimizer: optim.Optimizer, loss_fn: nn.Module, scheduler: optim.lr_scheduler._LRScheduler = None):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = model(\n",
    "        context_x=batch['context_x'],\n",
    "        context_y=batch['context_y'],\n",
    "        query_x=batch['query_x'],\n",
    "    )\n",
    "\n",
    "    target = batch['query_y'].float().view(-1, 1) \n",
    "    loss = loss_fn(logits, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float()\n",
    "        acc = (preds == target).float().mean().item()\n",
    "\n",
    "    return loss.item(), acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_step(model: nn.Module, batch: dict, loss_fn: nn.Module):\n",
    "    model.eval()\n",
    "\n",
    "    logits = model(\n",
    "        context_x=batch['context_x'],\n",
    "        context_y=batch['context_y'],\n",
    "        query_x=batch['query_x'],\n",
    "    )\n",
    "\n",
    "    target = batch['query_y'].float().view(-1, 1)\n",
    "    loss = loss_fn(logits, target)\n",
    "    \n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > 0.5).float()\n",
    "    acc = (preds == target).float().mean().item()\n",
    "\n",
    "    return loss.item(), acc, logits.cpu(), target.cpu()\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "        model: nn.Module, dataloader: DataLoader, optimizer: optim.Optimizer = None, scheduler: optim.lr_scheduler._LRScheduler = None\n",
    "    ) -> dict:\n",
    "    is_train = optimizer is not None\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_logits, all_labels = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        if is_train:\n",
    "            loss, acc = train_step(model, batch, optimizer, loss_fn, scheduler)\n",
    "            logits = None\n",
    "            labels = None\n",
    "        else:\n",
    "            loss, acc, logits, labels = eval_step(model, batch, loss_fn)\n",
    "            all_logits.append(logits)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "        B = batch['query_y'].numel()\n",
    "        total_loss += loss * B\n",
    "        correct += int(acc * B)\n",
    "        total += B\n",
    "\n",
    "    metrics = {\n",
    "        'loss': total_loss / total,\n",
    "        'acc': correct / total,\n",
    "    }\n",
    "\n",
    "    if not is_train:\n",
    "        metrics['logits'] = torch.cat(all_logits)\n",
    "        metrics['labels'] = torch.cat(all_labels)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    "    save_best: bool = False,\n",
    ") -> tuple[list[dict], dict | None]:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=lr, \n",
    "        epochs=epochs, \n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,  # Warmup for 30% of time\n",
    "    )\n",
    "\n",
    "    history = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_metrics = run_epoch(model, train_loader, optimizer, scheduler)\n",
    "        val_metrics = run_epoch(model, val_loader)\n",
    "\n",
    "        # Store metrics for this epoch\n",
    "        history.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_metrics['loss'],\n",
    "            'train_acc': train_metrics['acc'],\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_acc': val_metrics['acc']\n",
    "        })\n",
    "\n",
    "        if save_best and val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            best_state = {\n",
    "                k: v.detach().cpu()\n",
    "                for k, v in model.state_dict().items()\n",
    "            }\n",
    "\n",
    "        logging.info(\n",
    "            f\"[Epoch {epoch:03d}] \"\n",
    "            f\"Train loss={train_metrics['loss']:.4f}, acc={train_metrics['acc']:.3f} | \"\n",
    "            f\"Val loss={val_metrics['loss']:.4f}, acc={val_metrics['acc']:.3f}\"\n",
    "        )\n",
    "\n",
    "    return history, best_state\n",
    "\n",
    "\n",
    "def run_multiseed_experiment(\n",
    "    model_class: type[nn.Module],\n",
    "    model_kwargs: dict,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    seeds: list[int] | None,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    "    device: torch.device = torch.device('cpu'),\n",
    "    experiment_name: str = 'task',\n",
    "    checkpoint_dir: str | Path = 'checkpoints',\n",
    "    data_output_dir: str | Path = 'results',\n",
    ") -> pd.DataFrame:\n",
    "    if seeds is None:\n",
    "        seeds = [0, 1, 2]\n",
    "\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    data_output_dir = Path(data_output_dir)\n",
    "\n",
    "    all_results = []\n",
    "    model_name = model_class.__name__\n",
    "\n",
    "    logging.info(f'Running experiment for model: {model_name} with seeds: {seeds}')\n",
    "    \n",
    "    for seed in seeds:\n",
    "        logging.info(f'Starting training with seed: {seed}')\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Re-initialize Model Fresh \n",
    "        model = model_class(**model_kwargs).to(device)\n",
    "        history, best_state = train_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            save_best=True,\n",
    "        )\n",
    "\n",
    "        # Save best model checkpoint\n",
    "        ckpt_path = checkpoint_dir / experiment_name / f'{model_name}_seed{seed}.pt'\n",
    "        os.makedirs(ckpt_path.parent, exist_ok=True)\n",
    "        torch.save({\n",
    "            'model_class': model_name,\n",
    "            'model_kwargs': model_kwargs,\n",
    "            'seed': seed,\n",
    "            'best_state_dict': best_state,\n",
    "        }, ckpt_path)\n",
    "        logging.info(f'Saved checkpoint to {ckpt_path}')\n",
    "\n",
    "        # Add metadata and store\n",
    "        for epoch_data in history:\n",
    "            epoch_data.update({\n",
    "                'seed': seed,\n",
    "                'model': model_name,\n",
    "                'experiment': experiment_name,\n",
    "            })\n",
    "            all_results.append(epoch_data)\n",
    "\n",
    "    # Restore global seed\n",
    "    set_seed(0) \n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    # Save results to CSV\n",
    "    data_output_dir = Path(data_output_dir)\n",
    "    data_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    csv_path = data_output_dir / experiment_name / f'{model_name}_results.csv'\n",
    "    os.makedirs(csv_path.parent, exist_ok=True)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    logging.info(f'Saved results to {csv_path}')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b2afc",
   "metadata": {},
   "source": [
    "## Main Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad361953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLTransformer(nn.Module):\n",
    "    def __init__(self, d_in, n_ctx, d_model=128, n_layers=2, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_ctx = n_ctx\n",
    "        # Embeddings\n",
    "        self.x_proj = nn.Linear(d_in, d_model)\n",
    "        self.y_proj = nn.Linear(1, d_model)\n",
    "        self.query_proj = nn.Linear(d_in, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, n_ctx + 1, d_model) * 0.02)\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=4*d_model, \n",
    "            dropout=0.0, # No dropout for synthetic tasks required\n",
    "            batch_first=True,\n",
    "            activation='gelu',\n",
    "            norm_first=False,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, context_x, context_y, query_x):\n",
    "        B = context_x.shape[0]\n",
    "        N = context_x.shape[1]\n",
    "        # Embed Context: Combine x and y\n",
    "        ctx_emb = self.x_proj(context_x) + self.y_proj(context_y.unsqueeze(-1).float())\n",
    "        # Embed Query: It has no y, so we just embed x\n",
    "        q_emb = self.query_proj(query_x).unsqueeze(1) # [B, 1, D]\n",
    "        # Concatenate: [Ctx_1, ..., Ctx_N, Query]\n",
    "        seq = torch.cat([ctx_emb, q_emb], dim=1) # [B, N+1, D]\n",
    "        # Add Positional Embeddings\n",
    "        seq = seq + self.pos_emb[:, :seq.shape[1], :]\n",
    "        # Transformer Pass\n",
    "        out = self.transformer(seq)\n",
    "        # Predict only on the Query token (the last one)\n",
    "        query_out = out[:, -1, :]\n",
    "        logits = self.head(query_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b73448",
   "metadata": {},
   "source": [
    "## PART I: Recovery of Optimal Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Task A (In sample validation)\n",
    "run_multiseed_experiment(\n",
    "    model_class=ICLTransformer,\n",
    "    model_kwargs={\n",
    "        'd_in': data_cfg.d,\n",
    "        'n_ctx': data_cfg.n_ctx,\n",
    "        'd_model': 128,\n",
    "        'n_layers': 2,\n",
    "        'n_heads': 4,\n",
    "    },\n",
    "    train_loader=train_loader_A,\n",
    "    val_loader=val_loader_A,\n",
    "    seeds=train_cfg.seeds,\n",
    "    epochs=train_cfg.epochs,\n",
    "    lr=train_cfg.lr,\n",
    "    device=train_cfg.device,\n",
    "    experiment_name='task_A_regular',\n",
    "    checkpoint_dir='checkpoints',\n",
    "    data_output_dir='results',\n",
    ")\n",
    "\n",
    "# Experiment: Task A (OOD validation)\n",
    "run_multiseed_experiment(\n",
    "    model_class=ICLTransformer,\n",
    "    model_kwargs={\n",
    "        'd_in': data_cfg.d,\n",
    "        'n_ctx': data_cfg.n_ctx,\n",
    "        'd_model': 128,\n",
    "        'n_layers': 2,\n",
    "        'n_heads': 4,\n",
    "    },\n",
    "    train_loader=train_loader_A,\n",
    "    val_loader=val_loader_A_OOD,\n",
    "    seeds=train_cfg.seeds,\n",
    "    epochs=train_cfg.epochs,\n",
    "    lr=train_cfg.lr,\n",
    "    device=train_cfg.device,\n",
    "    experiment_name='task_A_OOD',\n",
    "    checkpoint_dir='checkpoints',\n",
    "    data_output_dir='results',\n",
    ")\n",
    "\n",
    "# Experiment: Task B\n",
    "run_multiseed_experiment(\n",
    "    model_class=ICLTransformer,\n",
    "    model_kwargs={\n",
    "        'd_in': data_cfg.d,\n",
    "        'n_ctx': data_cfg.n_ctx,\n",
    "        'd_model': 128,\n",
    "        'n_layers': 2,\n",
    "        'n_heads': 4,\n",
    "    },\n",
    "    train_loader=train_loader_B,\n",
    "    val_loader=val_loader_B,\n",
    "    seeds=train_cfg.seeds,\n",
    "    epochs=train_cfg.epochs,\n",
    "    lr=train_cfg.lr,\n",
    "    device=train_cfg.device,\n",
    "    experiment_name='task_B_regular',\n",
    "    checkpoint_dir='checkpoints',\n",
    "    data_output_dir='results',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846689ab",
   "metadata": {},
   "source": [
    "## PART II: Failure Modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d6cb25",
   "metadata": {},
   "source": [
    "### Model Ablation Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07671379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLTransformerInterleavedEmbeddings(nn.Module):\n",
    "    \"\"\" \n",
    "    Context provided as (x, y) pairs interleaved in sequence.\n",
    "    Tests the inductive bias of using (x, y) together, which is broken by interleaving.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, n_ctx, d_model=128, n_layers=2, n_heads=4, max_len=512):\n",
    "        assert max_len >= 2 * n_ctx + 1, 'max_len must be at least 2*n_ctx + 1'\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings\n",
    "        # Projects input data x -> d_model\n",
    "        self.x_embed = nn.Linear(d_in, d_model)\n",
    "        # Embeds binary labels y -> d_model\n",
    "        self.y_embed = nn.Embedding(2, d_model)\n",
    "        # Learned positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model) * 0.02)\n",
    "        \n",
    "        # Transformer Encoder (GPT-style)\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=4*d_model, \n",
    "            dropout=0.0, # No dropout for synthetic tasks required\n",
    "            batch_first=True,\n",
    "            activation='gelu',\n",
    "            norm_first=False,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, context_x, context_y, query_x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            context_x: (B, N, d)\n",
    "            context_y: (B, N)\n",
    "            query_x:   (B, d)\n",
    "        \"\"\"\n",
    "        B, N, _ = context_x.shape\n",
    "        device = context_x.device\n",
    "        # Embed inputs \n",
    "        ctx_x_emb = self.x_embed(context_x)  # (B, N, d_model)\n",
    "        qry_x_emb = self.x_embed(query_x.unsqueeze(1))  # (B, 1, d_model)\n",
    "        ctx_y_emb = self.y_embed(context_y)  # (B, N, d_model)\n",
    "        # Interleave Sequence \n",
    "        # Sequence: [x1, y1, x2, y2, ..., xN, yN, x_query]\n",
    "        # Total length = 2*N + 1\n",
    "        seq_len = 2*N + 1\n",
    "        seq_emb = torch.zeros(B, seq_len, self.d_model, device=device)\n",
    "        # Evens are X, Odds are Y\n",
    "        seq_emb[:, 0:2*N:2, :] = ctx_x_emb\n",
    "        seq_emb[:, 1:2*N:2, :] = ctx_y_emb\n",
    "        # Last token is Query X\n",
    "        seq_emb[:, -1, :] = qry_x_emb.squeeze(1)\n",
    "        # Add Position & Forward\n",
    "        seq_emb = seq_emb + self.pos_embed[:, :seq_len, :]\n",
    "        out = self.transformer(seq_emb)\n",
    "        # Predict on last token (repr of x_query)\n",
    "        last_token = out[:, -1, :]\n",
    "        return self.head(last_token)\n",
    "    \n",
    "\n",
    "class ICLTransformerNoLabels(nn.Module):\n",
    "    \"\"\"\n",
    "    Context provided, but labels y are removed.\n",
    "    Tests whether performance relies on (x, y) pairing.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, n_ctx, d_model=128, n_layers=2, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.x_proj = nn.Linear(d_in, d_model)\n",
    "        self.query_proj = nn.Linear(d_in, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, n_ctx + 1, d_model) * 0.02)\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=0.0,\n",
    "            batch_first=True,\n",
    "            activation='gelu',\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, context_x, context_y, query_x):\n",
    "        ctx_emb = self.x_proj(context_x)\n",
    "        q_emb = self.query_proj(query_x).unsqueeze(1)\n",
    "        seq = torch.cat([ctx_emb, q_emb], dim=1)\n",
    "        seq = seq + self.pos_emb[:, :seq.shape[1], :]\n",
    "        out = self.transformer(seq)\n",
    "        return self.head(out[:, -1])\n",
    "\n",
    "\n",
    "class ICLTransformerShuffledLabels(ICLTransformer):\n",
    "    \"\"\"\n",
    "    Labels y are randomly permuted within each batch.\n",
    "    Preserves label distribution but breaks x-y association.\n",
    "    \"\"\"\n",
    "    def forward(self, context_x, context_y, query_x):\n",
    "        B, N = context_y.shape\n",
    "        perm = torch.randperm(N, device=context_y.device)\n",
    "        shuffled_y = context_y[:, perm]\n",
    "        return super().forward(context_x, shuffled_y, query_x)\n",
    "\n",
    "\n",
    "class ICLTransformerShuffledContext(ICLTransformer):\n",
    "    \"\"\"\n",
    "    Context (x, y) pairs are randomly permuted within each batch.\n",
    "    Tests whether the model relies on the order of context points.\n",
    "    \"\"\"\n",
    "    def forward(self, context_x, context_y, query_x):\n",
    "        B, N, _ = context_x.shape\n",
    "        perm = torch.randperm(N, device=context_x.device)\n",
    "        return super().forward(\n",
    "            context_x[:, perm],\n",
    "            context_y[:, perm],\n",
    "            query_x\n",
    "        )\n",
    "    \n",
    "\n",
    "class ICLTransformerFrozenAttention(ICLTransformer):\n",
    "    \"\"\"\n",
    "    Attention weights are frozen at initialization.\n",
    "    Only value projections + MLPs train.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        for layer in self.transformer.layers:\n",
    "            attn = layer.self_attn\n",
    "\n",
    "            # Freeze Q, K, V projections\n",
    "            for p in attn.in_proj_weight, attn.in_proj_bias:\n",
    "                if p is not None:\n",
    "                    p.requires_grad = False\n",
    "\n",
    "            # Freeze output projection\n",
    "            attn.out_proj.weight.requires_grad = False\n",
    "            attn.out_proj.bias.requires_grad = False\n",
    "\n",
    "\n",
    "class ICLTransformerFrozenQK(ICLTransformer):\n",
    "    \"\"\"\n",
    "    Query and key projections frozen; values trainable.\n",
    "    Tests whether matching is essential or only aggregation.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        for layer in self.transformer.layers:\n",
    "            attn = layer.self_attn\n",
    "            d = attn.embed_dim\n",
    "            \n",
    "            # We define a hook that zeroes out the gradients for the first 2*d rows\n",
    "            # (which correspond to Q and K in the fused in_proj_weight)\n",
    "            def get_qk_freeze_hook(d_dim):\n",
    "                def hook(grad):\n",
    "                    # grad shape is (3*d, d) for weight, (3*d) for bias\n",
    "                    # We clone to ensure we don't modify the gradient buffer in place unexpectedly\n",
    "                    new_grad = grad.clone()\n",
    "                    # Zero out Q and K parts\n",
    "                    new_grad[:2*d_dim] = 0.0\n",
    "                    return new_grad\n",
    "                return hook\n",
    "\n",
    "            # Register the hook on the parameters\n",
    "            attn.in_proj_weight.register_hook(get_qk_freeze_hook(d))\n",
    "            \n",
    "            if attn.in_proj_bias is not None:\n",
    "                attn.in_proj_bias.register_hook(get_qk_freeze_hook(d))\n",
    "\n",
    "\n",
    "class ICLTransformerFrozenPos(ICLTransformer):\n",
    "    \"\"\"\n",
    "    Positional embeddings are frozen at initialization.\n",
    "    Tests reliance on learned absolute position.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pos_emb.requires_grad = False\n",
    "\n",
    "\n",
    "class ICLTransformerNoPos(ICLTransformer):\n",
    "    \"\"\"\n",
    "    No positional information at all.\n",
    "    Tests permutation-invariant aggregation.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        with torch.no_grad():\n",
    "            self.pos_emb.zero_()\n",
    "        self.pos_emb.requires_grad = False\n",
    "\n",
    "\n",
    "class ICLTransformerNoisyLabels(ICLTransformer):\n",
    "    \"\"\"\n",
    "    Injects random label noise during forward pass.\n",
    "    Tests robustness of evidence aggregation.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, noise_p=0.2, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.noise_p = noise_p\n",
    "\n",
    "    def forward(self, context_x, context_y, query_x):\n",
    "        if self.training and self.noise_p > 0:\n",
    "            noise = torch.rand_like(context_y.float()) < self.noise_p\n",
    "            context_y = context_y.clone()\n",
    "            context_y[noise] = 1 - context_y[noise]\n",
    "        return super().forward(context_x, context_y, query_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8032a4a",
   "metadata": {},
   "source": [
    "### Ablation Runs on Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf6b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABLATIONS_TASK_A = [\n",
    "    ('interleaved', ICLTransformerInterleavedEmbeddings, {}),\n",
    "    ('no_labels', ICLTransformerNoLabels, {}),\n",
    "    ('shuffled_labels', ICLTransformerShuffledLabels, {}),\n",
    "    ('shuffled_context', ICLTransformerShuffledContext, {}),\n",
    "    ('frozen_attention', ICLTransformerFrozenAttention, {}),\n",
    "    ('frozen_qk', ICLTransformerFrozenQK, {}),\n",
    "    ('frozen_pos', ICLTransformerFrozenPos, {}),\n",
    "    ('no_pos', ICLTransformerNoPos, {}),\n",
    "]\n",
    "for ablation_name, model_class, model_kwargs in ABLATIONS_TASK_A:\n",
    "    run_multiseed_experiment(\n",
    "        model_class=model_class,\n",
    "        model_kwargs={\n",
    "            'd_in': data_cfg.d,\n",
    "            'n_ctx': data_cfg.n_ctx,\n",
    "            'd_model': 128,\n",
    "            'n_layers': 2,\n",
    "            'n_heads': 4,\n",
    "            **model_kwargs,\n",
    "        },\n",
    "        train_loader=train_loader_A,\n",
    "        val_loader=val_loader_A,\n",
    "        seeds=train_cfg.seeds,\n",
    "        epochs=train_cfg.epochs,\n",
    "        lr=train_cfg.lr,\n",
    "        device=train_cfg.device,\n",
    "        experiment_name=f'task_A_ablation_{ablation_name}',\n",
    "        checkpoint_dir='checkpoints',\n",
    "        data_output_dir='results',\n",
    "    )\n",
    "\n",
    "\n",
    "run_multiseed_experiment(\n",
    "    model_class=ICLTransformer,\n",
    "    model_kwargs={\n",
    "        'd_in': data_cfg.d,\n",
    "        'n_ctx': 2 * data_cfg.n_ctx,\n",
    "        'd_model': 128,\n",
    "        'n_layers': 2,\n",
    "        'n_heads': 4,\n",
    "    },\n",
    "    train_loader=train_loader_A,\n",
    "    val_loader=val_loader_A,\n",
    "    seeds=train_cfg.seeds,\n",
    "    epochs=train_cfg.epochs,\n",
    "    lr=train_cfg.lr,\n",
    "    device=train_cfg.device,\n",
    "    experiment_name='task_A_ablation_context_increase',\n",
    "    checkpoint_dir='checkpoints',\n",
    "    data_output_dir='results',\n",
    ")\n",
    "\n",
    "\n",
    "NOISE_LEVELS = [0.1, 0.2, 0.4]\n",
    "for p in NOISE_LEVELS:\n",
    "    run_multiseed_experiment(\n",
    "        model_class=ICLTransformerNoisyLabels,\n",
    "        model_kwargs={\n",
    "            'd_in': data_cfg.d,\n",
    "            'n_ctx': data_cfg.n_ctx,\n",
    "            'd_model': 128,\n",
    "            'n_layers': 2,\n",
    "            'n_heads': 4,\n",
    "            'noise_p': p,\n",
    "        },\n",
    "        train_loader=train_loader_A,\n",
    "        val_loader=val_loader_A,\n",
    "        seeds=train_cfg.seeds,\n",
    "        epochs=train_cfg.epochs,\n",
    "        lr=train_cfg.lr,\n",
    "        device=train_cfg.device,\n",
    "        experiment_name=f'task_A_ablation_noisy_labels_p{p}',\n",
    "        checkpoint_dir='checkpoints',\n",
    "        data_output_dir='results',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da1c32",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Load all result CSVs\n",
    "# -----------------------------\n",
    "def load_all_results_csvs(results_root=\"results\"):\n",
    "    paths = sorted(glob.glob(os.path.join(results_root, \"**\", \"*.csv\"), recursive=True))\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] failed to read {p}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df[\"source_path\"] = p\n",
    "\n",
    "        if \"experiment\" not in df.columns:\n",
    "            df[\"experiment\"] = os.path.basename(os.path.dirname(p))\n",
    "\n",
    "        if \"seed\" in df.columns:\n",
    "            df[\"seed\"] = df[\"seed\"].astype(int)\n",
    "        if \"epoch\" in df.columns:\n",
    "            df[\"epoch\"] = df[\"epoch\"].astype(int)\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        raise RuntimeError(f\"No CSVs found under: {results_root}\")\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Mean ± 95% CI helper\n",
    "# -----------------------------\n",
    "def mean_ci95_halfwidth(x: pd.Series):\n",
    "    x = x.dropna().to_numpy(dtype=float)\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return np.nan, np.nan, 0\n",
    "    mean = float(np.mean(x))\n",
    "    if n == 1:\n",
    "        return mean, np.nan, 1\n",
    "    sd = float(np.std(x, ddof=1))\n",
    "    half = 1.96 * sd / math.sqrt(n)\n",
    "    return mean, half, n\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Final accuracy table (train + val, no halfwidth columns)\n",
    "# -----------------------------\n",
    "def final_train_val_table(df_all: pd.DataFrame):\n",
    "    required = {\"experiment\", \"model\", \"seed\", \"epoch\", \"train_acc\", \"val_acc\"}\n",
    "    missing = required - set(df_all.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    last_rows = (\n",
    "        df_all.sort_values([\"experiment\", \"model\", \"seed\", \"epoch\"])\n",
    "              .groupby([\"experiment\", \"model\", \"seed\"], as_index=False)\n",
    "              .tail(1)\n",
    "    )\n",
    "\n",
    "    out = []\n",
    "    for (exp, model), g in last_rows.groupby([\"experiment\", \"model\"]):\n",
    "        tr_m, tr_h, n1 = mean_ci95_halfwidth(g[\"train_acc\"])\n",
    "        va_m, va_h, n2 = mean_ci95_halfwidth(g[\"val_acc\"])\n",
    "        n = min(n1, n2)\n",
    "\n",
    "        out.append({\n",
    "            \"experiment\": exp,\n",
    "            \"model\": model,\n",
    "            \"n_seeds\": n,\n",
    "            \"train_acc_ci95\": (f\"{tr_m:.4f} ± {tr_h:.4f}\" if not np.isnan(tr_h) else f\"{tr_m:.4f}\"),\n",
    "            \"val_acc_ci95\":   (f\"{va_m:.4f} ± {va_h:.4f}\" if not np.isnan(va_h) else f\"{va_m:.4f}\"),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(out).sort_values([\"experiment\", \"model\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Plot train/val curves with 95% CI for one task\n",
    "# -----------------------------\n",
    "def plot_learning_curves_ci95(\n",
    "    df_all: pd.DataFrame,\n",
    "    experiment: str,\n",
    "    model: str | None = None,\n",
    "    out_path: str | None = None,\n",
    "):\n",
    "    required = {\"experiment\", \"epoch\", \"seed\", \"train_acc\", \"val_acc\"}\n",
    "    missing = required - set(df_all.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    df = df_all[df_all[\"experiment\"] == experiment].copy()\n",
    "    if model is not None:\n",
    "        if \"model\" not in df.columns:\n",
    "            raise ValueError(\"model column missing; cannot filter by model.\")\n",
    "        df = df[df[\"model\"] == model].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No rows found for experiment={experiment} model={model}\")\n",
    "\n",
    "    def agg_mean_ci(x):\n",
    "        x = x.dropna().to_numpy(dtype=float)\n",
    "        n = len(x)\n",
    "        m = float(np.mean(x)) if n else np.nan\n",
    "        if n <= 1:\n",
    "            return pd.Series({\"mean\": m, \"ci\": np.nan})\n",
    "        sd = float(np.std(x, ddof=1))\n",
    "        ci = 1.96 * sd / math.sqrt(n)\n",
    "        return pd.Series({\"mean\": m, \"ci\": ci})\n",
    "\n",
    "    train_stats = df.groupby(\"epoch\")[\"train_acc\"].apply(agg_mean_ci).reset_index()\n",
    "    val_stats   = df.groupby(\"epoch\")[\"val_acc\"].apply(agg_mean_ci).reset_index()\n",
    "\n",
    "    # flatten output (robust across pandas versions)\n",
    "    train_stats = train_stats.pivot(index=\"epoch\", columns=\"level_1\", values=\"train_acc\").reset_index()\n",
    "    val_stats   = val_stats.pivot(index=\"epoch\", columns=\"level_1\", values=\"val_acc\").reset_index()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_stats[\"epoch\"], train_stats[\"mean\"], label=\"train acc (mean)\")\n",
    "    plt.fill_between(\n",
    "        train_stats[\"epoch\"],\n",
    "        train_stats[\"mean\"] - train_stats[\"ci\"].fillna(0),\n",
    "        train_stats[\"mean\"] + train_stats[\"ci\"].fillna(0),\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "    plt.plot(val_stats[\"epoch\"], val_stats[\"mean\"], label=\"val acc (mean)\")\n",
    "    plt.fill_between(\n",
    "        val_stats[\"epoch\"],\n",
    "        val_stats[\"mean\"] - val_stats[\"ci\"].fillna(0),\n",
    "        val_stats[\"mean\"] + val_stats[\"ci\"].fillna(0),\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "    title = f\"{experiment}\" + (f\" | {model}\" if model is not None else \"\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.legend()\n",
    "\n",
    "    if out_path is not None:\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"Saved plot to: {out_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# 5) Run analysis\n",
    "df_all = load_all_results_csvs(\"results\")\n",
    "\n",
    "summary = final_train_val_table(df_all)\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "os.makedirs(\"analysis_out\", exist_ok=True)\n",
    "summary.to_csv(\"analysis_out/final_train_val_acc_ci95.csv\", index=False)\n",
    "print(\"Saved summary to analysis_out/final_train_val_acc_ci95.csv\")\n",
    "\n",
    "experiment_to_plot = \"task_A_regular\"\n",
    "model_to_plot = \"ICLTransformer\"\n",
    "\n",
    "plot_learning_curves_ci95(\n",
    "    df_all,\n",
    "    experiment=experiment_to_plot,\n",
    "    model=model_to_plot,\n",
    "    out_path=\"analysis_out/task_A_regular_learning_curve_ci95.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all experiments and models with available data\n",
    "experiment_to_plot = \"task_A_regular\"\n",
    "model_to_plot = \"ICLTransformer\"\n",
    "\n",
    "experiments = df_all[\"experiment\"].unique()\n",
    "models = df_all[\"model\"].unique()\n",
    "for exp in experiments:\n",
    "    for mod in models:\n",
    "        try:\n",
    "            plot_learning_curves_ci95(\n",
    "                df_all,\n",
    "                experiment=exp,\n",
    "                model=mod,\n",
    "                out_path=f\"analysis_out/{exp}_{mod}_learning_curve_ci95.png\",\n",
    "            )\n",
    "        except ValueError:\n",
    "            # Skip if no data for this combination\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121098a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfbda5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(ckpt_path, device='cpu'):\n",
    "    # 1. Load the file\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    \n",
    "    # 2. Get the class name and arguments\n",
    "    class_name = checkpoint['model_class']\n",
    "    kwargs = checkpoint['model_kwargs']\n",
    "    state_dict = checkpoint['best_state_dict']\n",
    "    \n",
    "    print(f\"Loading {class_name} from seed {checkpoint['seed']} from {ckpt_path}\")\n",
    "    \n",
    "    # 3. Map string name to actual Class object\n",
    "    # (Add any new ablation classes to this map if needed)\n",
    "    MODEL_MAP = {\n",
    "        'ICLTransformer': ICLTransformer,\n",
    "        'ICLTransformerNoLabels': ICLTransformerNoLabels,\n",
    "        'ICLTransformerFrozenPos': ICLTransformerFrozenPos,\n",
    "        'ICLTransformerNoPos': ICLTransformerNoPos,\n",
    "        'ICLTransformerFrozenQK': ICLTransformerFrozenQK,\n",
    "        'ICLTransformerShuffledLabels': ICLTransformerShuffledLabels,\n",
    "        'ICLTransformerShuffledContext': ICLTransformerShuffledContext,\n",
    "        'ICLTransformerNoisyLabels': ICLTransformerNoisyLabels,\n",
    "    }\n",
    "    \n",
    "    if class_name not in MODEL_MAP:\n",
    "        raise ValueError(f\"Unknown model class: {class_name}\")\n",
    "    \n",
    "    ModelClass = MODEL_MAP[class_name]\n",
    "    \n",
    "    # 4. Re-instantiate the model\n",
    "    model = ModelClass(**kwargs)\n",
    "    \n",
    "    # 5. Load weights\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval() # Set to eval mode by default\n",
    "    \n",
    "    return model\n",
    "\n",
    "path = \"checkpoints/task_A_regular/ICLTransformer_seed0.pt\"\n",
    "model = load_model_from_checkpoint(path, device=train_cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a228ef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "implicit-likelihood-ratio-transformer (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
